{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "import math\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.296485 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.02\n",
      "================================================================================\n",
      "itpa mge ifs y mhlfsrlejmrkijo qzmtmgeldk r a sdsxpesei  iu hnw  wttpnyof mtaoeu\n",
      "ygobsyreyi vl  w u y  odignxcqvtiarb w rduwromnmeffkaxekhi urm elgnntail ttwtwaw\n",
      "dlas yr i   tkdauablarjnrli tw nnhfbieniro edk h y jcdjmege kulxb  ucfmfa ikh mh\n",
      "n oxwggsdbgfoet esha v  ajj o r   nuipuzhohrds  xzqkawwpa qsogqeakfqirsnzcaexwko\n",
      "zbaascnrkvtpotqii bedhofrpofmvl fboxk  esrqsqug tcheytefhxew lxctqjrudorhvim ren\n",
      "================================================================================\n",
      "Validation set perplexity: 20.20\n",
      "Average loss at step 100: 2.600938 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.27\n",
      "Validation set perplexity: 10.91\n",
      "Average loss at step 200: 2.246013 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.86\n",
      "Validation set perplexity: 8.94\n",
      "Average loss at step 300: 2.098167 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.62\n",
      "Validation set perplexity: 8.14\n",
      "Average loss at step 400: 1.997837 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.50\n",
      "Validation set perplexity: 7.78\n",
      "Average loss at step 500: 1.934459 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.44\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 600: 1.912216 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.32\n",
      "Validation set perplexity: 7.03\n",
      "Average loss at step 700: 1.859983 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.35\n",
      "Validation set perplexity: 6.62\n",
      "Average loss at step 800: 1.819852 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.91\n",
      "Validation set perplexity: 6.36\n",
      "Average loss at step 900: 1.831061 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.10\n",
      "Validation set perplexity: 6.28\n",
      "Average loss at step 1000: 1.824537 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "================================================================================\n",
      "quy in wele the heve kurioms sacke to zeno far thove furs the beeraf only dicle \n",
      "vered gredion devere and mubel the swore resiverat ble abviouilationary humamena\n",
      "vething the produmber gulsifil uppooplic is to herich for one nine fwubf sypen s\n",
      " as bas the entraet if fine one nine three gnch the sooul alpacce ffor and have \n",
      "ferentury five one four two b cent nine femma sed may aptyil lite oden onlicion \n",
      "================================================================================\n",
      "Validation set perplexity: 6.16\n",
      "Average loss at step 1100: 1.776669 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 5.88\n",
      "Average loss at step 1200: 1.750618 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 5.65\n",
      "Average loss at step 1300: 1.733882 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 5.67\n",
      "Average loss at step 1400: 1.747571 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Validation set perplexity: 5.54\n",
      "Average loss at step 1500: 1.736235 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 5.52\n",
      "Average loss at step 1600: 1.744937 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 1700: 1.710899 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 1800: 1.670241 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 5.33\n",
      "Average loss at step 1900: 1.646194 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 2000: 1.694922 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "================================================================================\n",
      "vest is porkensming octresting one nine zero eight gine intersisting reconsived \n",
      "s sine knot od set whish he best in the kod but tespec was discince sfisting wen\n",
      "veriengry at the inclosing wolling the unigurg dain athall als is also we mance \n",
      "ket which who rols take cathiss admidding datem pershive yeasistents stints nort\n",
      "a arghinde candposition prodyctive the hoder and truicimation of hodore with of \n",
      "================================================================================\n",
      "Validation set perplexity: 5.38\n",
      "Average loss at step 2100: 1.685305 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 2200: 1.679844 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.36\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 2300: 1.637519 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 2400: 1.659329 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 2500: 1.675785 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 2600: 1.652239 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 2700: 1.651445 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 2800: 1.645065 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 2900: 1.647321 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 3000: 1.653651 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "================================================================================\n",
      "y and imply offalilitan apajlics witha are appetid bemonian octron uscairi of to\n",
      "viated procesustitue at fuppumple extulled patist call texturvles as telerisayia\n",
      "kle anconcy yowleds itels spanic mapa to replacy one six eight six adasticly for\n",
      "ricamed zero proditia ponsion in bntrome streated and such empressive it all get\n",
      "entse wouldphes impod standstralides and the knood and partises can admrrictors \n",
      "================================================================================\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 3100: 1.625549 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 3200: 1.640577 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 3300: 1.637385 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 3400: 1.660942 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 3500: 1.654818 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 3600: 1.665721 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 3700: 1.641130 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 3800: 1.643693 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 3900: 1.635341 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 4000: 1.648414 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.70\n",
      "================================================================================\n",
      " most colo deatan byfelolech central in returloric force yezion copel presate sa\n",
      "dezing metshole others the ruszioury used that paoter in composer trogetaboloh m\n",
      "quied zereio demotals the bejects in notel of congrazer solition compressing the\n",
      "le mashing broxsholic but for basblan operated algued austribed mone highe zeala\n",
      "ker a ancorbalrizae all cotlittal other remetres auture extems in the shing and \n",
      "================================================================================\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 4100: 1.627946 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 4200: 1.634089 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 4300: 1.613090 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 4400: 1.609443 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 4500: 1.611549 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 4600: 1.612461 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 4700: 1.618697 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 4800: 1.628537 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.29\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 4900: 1.629663 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 5000: 1.604354 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.47\n",
      "================================================================================\n",
      " in two zero two five the cults as a forts used ickoseme copperide their jan so \n",
      "org a cale de libestible mine ligender never gebered untest ho withir ratingring\n",
      "ling cripw one six excented as is scient peblikes in not zomferech concell mubii\n",
      "al bell whili is the myshowad jublith single great states l qualter gaph gef sol\n",
      "wowks types bracks america dectionitieal in englin it weirhibh six undeing that \n",
      "================================================================================\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 5100: 1.600909 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 5200: 1.586830 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 5300: 1.576536 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 5400: 1.578644 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 5500: 1.568332 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5600: 1.582285 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 5700: 1.565959 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5800: 1.579085 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 5900: 1.572593 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 6000: 1.545360 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "================================================================================\n",
      "ver one seven five two zero zero three two two one six nine twi one nine eight s\n",
      "oup attimmal acu othese groin history kardam one five five eight charoce had by \n",
      "al a view the relation american aruaispal rebolise in sealement cotses with lead\n",
      "zanian to it was sealge man on veryoftine which istendist seee voo iacion is hea\n",
      " and wordt englard commanda tabnil in the emitrict as can of s creationsique hum\n",
      "================================================================================\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 6100: 1.563373 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6200: 1.534631 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 6300: 1.541420 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 6400: 1.540976 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6500: 1.554128 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6600: 1.592292 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6700: 1.577460 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6800: 1.598987 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 6900: 1.581304 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 7000: 1.572638 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "================================================================================\n",
      "lay was ebscieffrealed king ohten in six the vilg a descrited to int the remolan\n",
      "diepofic roff woiled success bemals of which well sharcially wing varymet rack i\n",
      "ono genest in not manous also the entrict loskolith arm relations sucg on inscib\n",
      "phort y lines with eurond the one seven six s had a three two seven seven seven \n",
      "plyst isia one eight eight three seven amenited for interral lecary sacradi d of\n",
      "================================================================================\n",
      "Validation set perplexity: 4.30\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocabulary_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-bed383d97826>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;31m# Parameters:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0mx4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtruncated_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvocabulary_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_nodes\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0mm4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtruncated_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum_nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_nodes\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mb4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_nodes\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vocabulary_size' is not defined"
     ]
    }
   ],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  x4 = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes*4], -0.1, 0.1))\n",
    "  m4 = tf.Variable(tf.truncated_normal([num_nodes, num_nodes*4], -0.1, 0.1))\n",
    "  b4 = tf.Variable(tf.zeros([1, num_nodes*4]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    gate = tf.matmul(i, x4) + tf.matmul(o, m4) + b4\n",
    "    smatmul_input, smatmul_forget, update, smatmul_output  = tf.split(gate, num_or_size_splits=4, axis=1)\n",
    "    input_gate = tf.sigmoid(smatmul_input)\n",
    "    forget_gate = tf.sigmoid(smatmul_forget)\n",
    "    output_gate = tf.sigmoid(smatmul_output)\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "ae dr  j\n"
     ]
    }
   ],
   "source": [
    "l_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "vocabulary_size = l_size*l_size\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "vdict = {}\n",
    "reverse_vdict = {}\n",
    "\n",
    "for i in range(27):\n",
    "    for j in range(27):\n",
    "        char1 = ' '\n",
    "        char2 = ' '\n",
    "        if i > 0:\n",
    "            char1 = chr(i + first_letter - 1)\n",
    "        if j > 0:\n",
    "            char2 = chr(j + first_letter - 1)\n",
    "        dchar = str(char1)+str(char2)\n",
    "        vdict[dchar] = i*27+j\n",
    "        reverse_vdict[i*27+j] = dchar\n",
    "#print (vdict)\n",
    "#print (reverse_vdict)\n",
    "\n",
    "def dchar2id(dchar):\n",
    "    if dchar not in vdict:\n",
    "        print (\"undefined charaters\" + dchar)\n",
    "        return 0\n",
    "    else:\n",
    "        return vdict[dchar]\n",
    "\n",
    "def id2dchar(dictid):\n",
    "    if dictid not in reverse_vdict:\n",
    "        print (\"undefined reverse id\")\n",
    "        return \"  \"\n",
    "    else:\n",
    "        return reverse_vdict[dictid]\n",
    "print(dchar2id(\"aa\"))\n",
    "print(id2dchar(32), id2dchar(126), id2dchar(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, dchar2id(self._text[self._cursor[b]: self._cursor[b]+2])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2dchar(c)[0] for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-51-86a5038318aa>:55: calling argmax (from tensorflow.python.ops.math_ops) with dimension is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the `axis` argument instead\n",
      "WARNING:tensorflow:From <ipython-input-51-86a5038318aa>:55: calling argmax (from tensorflow.python.ops.math_ops) with dimension is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the `axis` argument instead\n",
      "WARNING:tensorflow:From <ipython-input-51-86a5038318aa>:55: calling argmax (from tensorflow.python.ops.math_ops) with dimension is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the `axis` argument instead\n",
      "WARNING:tensorflow:From <ipython-input-51-86a5038318aa>:55: calling argmax (from tensorflow.python.ops.math_ops) with dimension is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the `axis` argument instead\n",
      "WARNING:tensorflow:From <ipython-input-51-86a5038318aa>:55: calling argmax (from tensorflow.python.ops.math_ops) with dimension is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the `axis` argument instead\n",
      "WARNING:tensorflow:From <ipython-input-51-86a5038318aa>:55: calling argmax (from tensorflow.python.ops.math_ops) with dimension is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the `axis` argument instead\n",
      "WARNING:tensorflow:From <ipython-input-51-86a5038318aa>:55: calling argmax (from tensorflow.python.ops.math_ops) with dimension is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the `axis` argument instead\n",
      "WARNING:tensorflow:From <ipython-input-51-86a5038318aa>:55: calling argmax (from tensorflow.python.ops.math_ops) with dimension is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the `axis` argument instead\n",
      "WARNING:tensorflow:From <ipython-input-51-86a5038318aa>:55: calling argmax (from tensorflow.python.ops.math_ops) with dimension is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the `axis` argument instead\n",
      "WARNING:tensorflow:From <ipython-input-51-86a5038318aa>:55: calling argmax (from tensorflow.python.ops.math_ops) with dimension is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the `axis` argument instead\n",
      "WARNING:tensorflow:From <ipython-input-51-86a5038318aa>:84: calling argmax (from tensorflow.python.ops.math_ops) with dimension is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the `axis` argument instead\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Variables.\n",
    "  embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  # Look up embeddings for inputs.\n",
    "  #embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n",
    "\n",
    "  # Parameters:\n",
    "  x4 = tf.Variable(tf.truncated_normal([embedding_size, num_nodes*4], -0.1, 0.1))\n",
    "  m4 = tf.Variable(tf.truncated_normal([num_nodes, num_nodes*4], -0.1, 0.1))\n",
    "  b4 = tf.Variable(tf.zeros([1, num_nodes*4]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    gate = tf.matmul(i, x4) + tf.matmul(o, m4) + b4\n",
    "    smatmul_input, smatmul_forget, update, smatmul_output  = tf.split(gate, num_or_size_splits=4, axis=1)\n",
    "    input_gate = tf.sigmoid(smatmul_input)\n",
    "    forget_gate = tf.sigmoid(smatmul_forget)\n",
    "    output_gate = tf.sigmoid(smatmul_output)\n",
    "    tf.nn.dropout(input_gate, keep_prob)\n",
    "    tf.nn.dropout(output_gate, keep_prob)\n",
    "    tf.nn.dropout(update, keep_prob)\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    i_embed = tf.nn.embedding_lookup(embeddings, tf.argmax(i, dimension=1))\n",
    "    output, state = lstm_cell(i_embed, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_i = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  sample_input = tf.nn.embedding_lookup(embeddings, tf.argmax(sample_i, dimension=1))\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.608222 learning rate: 10.000000\n",
      "Minibatch perplexity: 741.16\n",
      "================================================================================\n",
      "otvlvv a giangrq tblltobet vndh nmidtjxkjpqnpeikpxsvnsoxhiguznagpghiepgmozcsjfbq\n",
      "yozezruqzjdbomxlrdpu cwugqjpoulwt lkzytixrhorcpfltfgkrazzqlx fdovezzehfgpthvmkkm\n",
      "wcpykstxikphlskpvokjbgok eppfgvhgzpexqlfqabkowqhmvdrczksptocleilsvxqeh ppsghgsxa\n",
      "lmescohgdokfsq etlojsfaualunjmvzzpcgwlix esmjelbkzft xqbkayqexwwpz kweikilg jfvw\n",
      "ysnmoahfbrnady lrojjzugea phpzwcfehzqjbxkhtzojimkyrpphszbijkyzlaglgpskhflrk llhw\n",
      "================================================================================\n",
      "undefined charatersi\n",
      "Validation set perplexity: 637.53\n",
      "Average loss at step 100: 3.401560 learning rate: 10.000000\n",
      "Minibatch perplexity: 14.06\n",
      "undefined charatersi\n",
      "Validation set perplexity: 10.73\n",
      "Average loss at step 200: 2.070550 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.87\n",
      "undefined charatersi\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 300: 1.871400 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "undefined charatersi\n",
      "Validation set perplexity: 6.45\n",
      "Average loss at step 400: 1.793767 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "undefined charatersi\n",
      "Validation set perplexity: 6.25\n",
      "Average loss at step 500: 1.791297 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "undefined charatersi\n",
      "Validation set perplexity: 5.85\n",
      "Average loss at step 600: 1.705788 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "undefined charatersi\n",
      "Validation set perplexity: 5.63\n",
      "Average loss at step 700: 1.680324 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "undefined charatersi\n",
      "Validation set perplexity: 5.53\n",
      "Average loss at step 800: 1.671692 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "undefined charatersi\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 900: 1.672765 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.32\n",
      "undefined charatersi\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 1000: 1.590358 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "================================================================================\n",
      "pros hard syserval formolizerly former pwinitary by epth bledfa is inflortse the\n",
      "mybood in one nine eight the eight one nia ush set tasking a find wiki sitive he\n",
      "mn the eastist popular briddieivid to greent or new togelie conceparloded or reg\n",
      "lmous one six on t zgle the day dicinment one zero one nine zero five two one ni\n",
      "ykring admid one nine six twof wingst from one nine seren one four d the mythern\n",
      "================================================================================\n",
      "undefined charatersi\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 1100: 1.578684 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "undefined charatersi\n",
      "Validation set perplexity: 5.47\n",
      "Average loss at step 1200: 1.604321 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "undefined charatersi\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 1300: 1.574298 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "undefined charatersi\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 1400: 1.560014 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.36\n",
      "undefined charatersi\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 1500: 1.553034 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "undefined charatersi\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 1600: 1.553516 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "undefined charatersi\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 1700: 1.584539 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "undefined charatersi\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 1800: 1.545113 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.79\n",
      "undefined charatersi\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 1900: 1.546440 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "undefined charatersi\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 2000: 1.557897 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.51\n",
      "================================================================================\n",
      "kton would win jesting toxiches as guns a docos ratellabate baseg freha the base\n",
      "cated and concreplese drivelves particken or two zero zero zero zero per the one\n",
      "mwardstonominated the gategorate braitadivitish each schowing number the us mess\n",
      "ik a massacre d reeigh that chongths a consekuvin tailar of road oring risection\n",
      "ok john blolled drugger dobady between electreted spacese papetime funce of god \n",
      "================================================================================\n",
      "undefined charatersi\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 2100: 1.549460 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.32\n",
      "undefined charatersi\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 2200: 1.523447 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "undefined charatersi\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 2300: 1.527101 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.36\n",
      "undefined charatersi\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 2400: 1.530301 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "undefined charatersi\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 2500: 1.557068 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "undefined charatersi\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 2600: 1.530162 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "undefined charatersi\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 2700: 1.541211 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.62\n",
      "undefined charatersi\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 2800: 1.501971 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.15\n",
      "undefined charatersi\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 2900: 1.505436 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.06\n",
      "undefined charatersi\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 3000: 1.507682 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "================================================================================\n",
      "ktimed spanish society of moved by teash couil rawspared to encourse the champ p\n",
      "ediums is a volution of cirectionards side the met advancis un cathemeted as abo\n",
      "mleop three germanue with several application e one one zero one one eight nine \n",
      "dritipes da state galonal a enserver among of he sources and a cm extralian the \n",
      "x this and other definitions for taxitimatey often one one five one four it cust\n",
      "================================================================================\n",
      "undefined charatersi\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 3100: 1.509132 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.79\n",
      "undefined charatersi\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 3200: 1.511977 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.67\n",
      "undefined charatersi\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 3300: 1.490038 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.06\n",
      "undefined charatersi\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 3400: 1.489389 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.23\n",
      "undefined charatersi\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 3500: 1.482639 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "undefined charatersi\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 3600: 1.479359 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.29\n",
      "undefined charatersi\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 3700: 1.476591 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.05\n",
      "undefined charatersi\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 3800: 1.474989 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.31\n",
      "undefined charatersi\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 3900: 1.476588 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.55\n",
      "undefined charatersi\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 4000: 1.473801 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.24\n",
      "================================================================================\n",
      "vstone comfloseries persting swell abbardleratistan bankned by anyord sharrison \n",
      "cssed by one zero koined other on defined they consequently issted formered th o\n",
      "net gased to burning observed used it is have on the angain west to utteen such \n",
      "ymals at on galace domosation or a gericism netmsed estably set five operation l\n",
      "z censorship living to them overwara into the work the traturature by his tass s\n",
      "================================================================================\n",
      "undefined charatersi\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 4100: 1.479807 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "undefined charatersi\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 4200: 1.454369 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.41\n",
      "undefined charatersi\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 4300: 1.450365 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.75\n",
      "undefined charatersi\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 4400: 1.473421 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.58\n",
      "undefined charatersi\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 4500: 1.487188 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.38\n",
      "undefined charatersi\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 4600: 1.486419 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.48\n",
      "undefined charatersi\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 4700: 1.460038 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.60\n",
      "undefined charatersi\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 4800: 1.441868 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.71\n",
      "undefined charatersi\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 4900: 1.456144 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.23\n",
      "undefined charatersi\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 5000: 1.480426 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.42\n",
      "================================================================================\n",
      "d graphy six for it as he markets and opponentar free set or scorial lendeving u\n",
      "gp at cagpt informal per but yet pacific globised ute of a based dnon b dustral \n",
      "sne a missile such and party cards of the stricting bu be uncent in stry only su\n",
      "ucially sponateory itself the balternalogan ca western two one three near two ma\n",
      "ging attack a globintains affected for the plays expotor destimmors to culf a to\n",
      "================================================================================\n",
      "undefined charatersi\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5100: 1.493080 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "undefined charatersi\n",
      "Validation set perplexity: 4.13\n",
      "Average loss at step 5200: 1.483810 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.80\n",
      "undefined charatersi\n",
      "Validation set perplexity: 4.07\n",
      "Average loss at step 5300: 1.443258 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.01\n",
      "undefined charatersi\n",
      "Validation set perplexity: 4.04\n",
      "Average loss at step 5400: 1.439898 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.26\n",
      "undefined charatersi\n",
      "Validation set perplexity: 4.04\n",
      "Average loss at step 5500: 1.433164 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.18\n",
      "undefined charatersi\n",
      "Validation set perplexity: 4.04\n",
      "Average loss at step 5600: 1.447704 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "undefined charatersi\n",
      "Validation set perplexity: 4.01\n",
      "Average loss at step 5700: 1.413281 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.51\n",
      "undefined charatersi\n",
      "Validation set perplexity: 4.05\n",
      "Average loss at step 5800: 1.423134 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.23\n",
      "undefined charatersi\n",
      "Validation set perplexity: 3.99\n",
      "Average loss at step 5900: 1.431431 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.84\n",
      "undefined charatersi\n",
      "Validation set perplexity: 3.98\n",
      "Average loss at step 6000: 1.403040 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "================================================================================\n",
      "zlcan sale in empgragne put the tradio substants of according that s tones a hig\n",
      "lounders of the islaudience or the develope a borrea sixtrantiney are concert di\n",
      "wens this was occur because one nine nine four experience would channels nate ti\n",
      "zly or was an oarlien cnd to so sunstartingual town the conquita of oil the cons\n",
      "hzly low time footh when h lurk if the mac demaled with carthage on the connecti\n",
      "================================================================================\n",
      "undefined charatersi\n",
      "Validation set perplexity: 4.04\n",
      "Average loss at step 6100: 1.418871 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.80\n",
      "undefined charatersi\n",
      "Validation set perplexity: 4.06\n",
      "Average loss at step 6200: 1.444768 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.24\n",
      "undefined charatersi\n",
      "Validation set perplexity: 4.03\n",
      "Average loss at step 6300: 1.458879 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "undefined charatersi\n",
      "Validation set perplexity: 4.00\n",
      "Average loss at step 6400: 1.491005 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "undefined charatersi\n",
      "Validation set perplexity: 3.93\n",
      "Average loss at step 6500: 1.481408 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.08\n",
      "undefined charatersi\n",
      "Validation set perplexity: 3.90\n",
      "Average loss at step 6600: 1.448282 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.23\n",
      "undefined charatersi\n",
      "Validation set perplexity: 3.90\n",
      "Average loss at step 6700: 1.438678 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.32\n",
      "undefined charatersi\n",
      "Validation set perplexity: 3.93\n",
      "Average loss at step 6800: 1.422300 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.85\n",
      "undefined charatersi\n",
      "Validation set perplexity: 3.96\n",
      "Average loss at step 6900: 1.415352 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "undefined charatersi\n",
      "Validation set perplexity: 3.94\n",
      "Average loss at step 7000: 1.431338 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.05\n",
      "================================================================================\n",
      "mhp live people on video bridgini an qvotia also in the had dnanachifts and vi c\n",
      "qtcalle of white pasinocky of there is mood jester one nine six nine degreen als\n",
      "dwanter c since recomple because of antimater b herlabn citize and degree sports\n",
      "pbere in fall in founder coalk while and an areas loves for think aucable that t\n",
      "troard a football space their an duncciating the showever burner to attentation \n",
      "================================================================================\n",
      "undefined charatersi\n",
      "Validation set perplexity: 3.90\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    feed_dict[keep_prob] = 0.5\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_i: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_i: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
